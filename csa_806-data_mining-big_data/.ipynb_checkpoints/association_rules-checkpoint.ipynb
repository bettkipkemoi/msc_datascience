{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71bab1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8fec864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file\n",
    "def read_transactions(path):\n",
    "\t\"\"\"Read the CSV where each row is a comma-separated transaction.\"\"\"\n",
    "\ttransactions = []\n",
    "\twith open(path, 'r', encoding='utf-8') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tparts = [p.strip() for p in line.strip().split(',') if p.strip()]\n",
    "\t\t\tif parts:\n",
    "\t\t\t\ttransactions.append(parts)\n",
    "\treturn transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ed955c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## eclat\n",
    "def eclat_bruteforce(transactions, min_support=0.05):\n",
    "\t\"\"\"Simple Eclat-style frequent itemset discovery by counting combinations.\n",
    "\tThis is a brute-force implementation suitable for small item counts.\n",
    "\tReturns a DataFrame with columns ['support', 'itemsets'].\n",
    "\t\"\"\"\n",
    "\tn = len(transactions)\n",
    "\t# unique items\n",
    "\titems = sorted({item for t in transactions for item in t})\n",
    "\tresults = []\n",
    "\tmax_k = len(items)\n",
    "\tfor k in range(1, max_k + 1):\n",
    "\t\tany_found = False\n",
    "\t\tfor combo in combinations(items, k):\n",
    "\t\t\tcount = 0\n",
    "\t\t\tcombo_set = set(combo)\n",
    "\t\t\tfor t in transactions:\n",
    "\t\t\t\tif combo_set.issubset(t):\n",
    "\t\t\t\t\tcount += 1\n",
    "\t\t\tsupport = count / n\n",
    "\t\t\tif support >= min_support:\n",
    "\t\t\t\tany_found = True\n",
    "\t\t\t\tresults.append({'support': support, 'itemsets': frozenset(combo)})\n",
    "\t\tif not any_found:\n",
    "\t\t\t# no frequent itemsets of this size -> larger sizes will not be frequent\n",
    "\t\t\tbreak\n",
    "\tif results:\n",
    "\t\tdf = pd.DataFrame(results).sort_values(by=['support', 'itemsets'], ascending=[False, True])\n",
    "\telse:\n",
    "\t\tdf = pd.DataFrame(columns=['support', 'itemsets'])\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c44fa629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading transactions from Breakfast_Transactions.csv\n",
      "Number of transactions: 605\n",
      "\n",
      "Apriori found 50 itemsets in 0.015s\n",
      "FPGrowth found 50 itemsets in 0.005s\n",
      "Eclat (bruteforce) found 50 itemsets in 0.047s\n",
      "\n",
      "Rules found:\n",
      "Apriori rules: 12\n",
      "FPGrowth rules: 12\n",
      "Eclat rules: 12\n",
      "\n",
      "Summary:\n",
      "Apriori: 50 itemsets, 12 rules, time=0.015s\n",
      "FPGrowth: 50 itemsets, 12 rules, time=0.005s\n",
      "Eclat: 50 itemsets, 12 rules, time=0.047s\n"
     ]
    }
   ],
   "source": [
    "## main function\n",
    "def main():\n",
    "\tdata_path = 'Breakfast_Transactions.csv'\n",
    "\tprint('Reading transactions from', data_path)\n",
    "\ttransactions = read_transactions(data_path)\n",
    "\tprint(f'Number of transactions: {len(transactions)}')\n",
    "\n",
    "\t# One-hot encode\n",
    "\tte = TransactionEncoder()\n",
    "\tte_ary = te.fit(transactions).transform(transactions)\n",
    "\tdf_onehot = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "\tmin_support = 0.05\n",
    "\tmin_confidence = 0.6\n",
    "\n",
    "\t# Apriori\n",
    "\tt0 = time.perf_counter()\n",
    "\tfi_apriori = apriori(df_onehot, min_support=min_support, use_colnames=True)\n",
    "\tt1 = time.perf_counter()\n",
    "\tapriori_time = t1 - t0\n",
    "\tprint('\\nApriori found', len(fi_apriori), 'itemsets in', f'{apriori_time:.3f}s')\n",
    "\tfi_apriori.to_csv('frequent_itemsets_apriori.csv', index=False)\n",
    "\n",
    "\t# FP-Growth\n",
    "\tt0 = time.perf_counter()\n",
    "\tfi_fpgrowth = fpgrowth(df_onehot, min_support=min_support, use_colnames=True)\n",
    "\tt1 = time.perf_counter()\n",
    "\tfpgrowth_time = t1 - t0\n",
    "\tprint('FPGrowth found', len(fi_fpgrowth), 'itemsets in', f'{fpgrowth_time:.3f}s')\n",
    "\tfi_fpgrowth.to_csv('frequent_itemsets_fpgrowth.csv', index=False)\n",
    "\n",
    "\t# Eclat (brute-force)\n",
    "\tt0 = time.perf_counter()\n",
    "\tfi_eclat = eclat_bruteforce(transactions, min_support=min_support)\n",
    "\tt1 = time.perf_counter()\n",
    "\teclat_time = t1 - t0\n",
    "\tprint('Eclat (bruteforce) found', len(fi_eclat), 'itemsets in', f'{eclat_time:.3f}s')\n",
    "\tfi_eclat.to_csv('frequent_itemsets_eclat.csv', index=False)\n",
    "\n",
    "\t# Generate association rules for apriori results\n",
    "\trules_apriori = association_rules(fi_apriori, metric='confidence', min_threshold=min_confidence)\n",
    "\trules_fpgrowth = association_rules(fi_fpgrowth, metric='confidence', min_threshold=min_confidence)\n",
    "\t# For eclat, association_rules expects a DataFrame with itemsets in 'itemsets' column; our df matches that\n",
    "\trules_eclat = association_rules(fi_eclat.rename(columns={'itemsets': 'itemsets', 'support': 'support'}),\n",
    "\t\t\t\t\t\t\t\t\tmetric='confidence', min_threshold=min_confidence) if not fi_eclat.empty else pd.DataFrame()\n",
    "\n",
    "\tprint('\\nRules found:')\n",
    "\tprint('Apriori rules:', len(rules_apriori))\n",
    "\tprint('FPGrowth rules:', len(rules_fpgrowth))\n",
    "\tprint('Eclat rules:', len(rules_eclat))\n",
    "\n",
    "\t# Save rules\n",
    "\trules_apriori.to_csv('rules_apriori.csv', index=False)\n",
    "\trules_fpgrowth.to_csv('rules_fpgrowth.csv', index=False)\n",
    "\trules_eclat.to_csv('rules_eclat.csv', index=False)\n",
    "\n",
    "\t# Summary\n",
    "\tprint('\\nSummary:')\n",
    "\tprint(f'Apriori: {len(fi_apriori)} itemsets, {len(rules_apriori)} rules, time={apriori_time:.3f}s')\n",
    "\tprint(f'FPGrowth: {len(fi_fpgrowth)} itemsets, {len(rules_fpgrowth)} rules, time={fpgrowth_time:.3f}s')\n",
    "\tprint(f'Eclat: {len(fi_eclat)} itemsets, {len(rules_eclat)} rules, time={eclat_time:.3f}s')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
