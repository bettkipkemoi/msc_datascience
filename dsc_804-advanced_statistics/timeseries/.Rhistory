#load libraries
library(tidyverse)
install.packages("tidyverse")
install.packages("rvest")
install.packages("data.table")
install.packages("rlist")
#load libraries
library(tidyverse)
library(rvest)
library(data.table)
library(rlist)
url <- 'https://www.yelp.com.au/search?find_desc=neon&find_loc=Melbourne%2C+Victoria%2C+Australia'
page <- read_html(url)
View(page)
page %>%
html_elements(xpath = "//div[starts-wth(@class, 'business-name')]")
page %>%
html_elements(xpath = "//div[starts-with(@class, 'business-name')]")
page %>%
html_elements(xpath = "//div[starts-with(@class, 'businessName')]") %>%
#load libraries
library(tidyverse)
library(rvest)
library(data.table)
library(rlist)
page %>%
html_elements(xpath = "//div[starts-with(@class, 'businessName')]")
page %>%
html_elements(xpath = "//div[starts-with(@class, 'businessName')]") %>%
html_elements(xpath = ".//a[starts-with(@class, '/biz')]")
page %>%
html_elements(xpath = "//div[starts-with(@class, 'businessName')]") %>%
html_elements(xpath = ".//a[starts-with(@class, '/biz/')]")
page %>%
html_elements(xpath = "//div[starts-with(@class, 'businessName')]") %>%
html_elements(xpath = ".//a[starts-with(@href, '/biz')]")
name <- page %>%
html_elements(xpath = "//div[starts-with(@class, 'businessName')]") %>%
html_elements(xpath = ".//a[starts-with(@href, '/biz')]") %>%
html_text()
phone <- page %>%
html_elements(xpath = "//div[starts-with(@class, 'biz-details')]")
page %>%
html_elements(xpath = "//div[starts-with(@class, 'biz-details-page')]")
page %>%
html_elements(xpath = "//p[starts-with(@class, ' css-1p9ibgf')]")
page %>%
html_elements(xpath = "//div[starts-with(@class, ' css-1p9ibgf')]")
page %>%
html_elements(xpath = "//div[starts-with(@class, 'biz')]")
url <- 'https://www.yellowpages.com.au/search/listings?clue=neon&locationClue=&lat=&lon='
page <- read_html(url)
#load libraries
library(tidyverse)
library(rvest)
library(data.table)
library(rlist)
url <- 'https://www.yellowpages.com.au/search/listings?clue=neon&locationClue=&lat=&lon='
page <- read_html(url)
name <- page %>%
html_elements(xpath = "//div[starts-with(@class, 'businessName')]") %>%
html_elements(xpath = ".//a[starts-with(@href, '/biz')]") %>%
html_text()
#load libraries
library(tidyverse)
library(rvest)
library(data.table)
library(rlist)
url <- 'https://www.yellowpages.com.au/search/listings?clue=neon&locationClue=&lat=&lon='
page <- read_html(url)
name <- page %>%
html_elements(xpath = "//div[starts-with(@class, 'businessName')]") %>%
html_elements(xpath = ".//a[starts-with(@href, '/biz')]") %>%
html_text()
install.packages("https://cloud.r-project.org/bin/windows/contrib/3.1/curl_0.9.7.zip", repos=NULL,type="source")
install.packages("curl")
close()
clear()
nsb
shots <- c(456, 345, 765, 478)
goals <- c(62, 65, 67, 63)
plot(shots, goals, xlab="shots", ylab="goals", main="shots vs goals scored")
clear
compensation_employees <- c(2774320)
unexplained <- c(2126300)
barplot(shots, goals, xlab="financial report", ylab="amount", main="variance in the financial report")
mean(c(4,3,4,NA,4),na.rm = TRUE)
library(dplyr)
View(iris)
#arranging data
iris_arr <- iris %>% arrange(desc(Sepal.Width))
View(iris_arr)
#filtering data
iris_filt <- iris %>% filter(Petal.Length == 4.0)
View(iris_filt)
View(iris_arr)
View(iris_arr)
#mutating data
iris_mut <- iris %>%
mutate(petal_perimeter = 2*Petal.Length + 2*Petal.Width)
View(iris_mut)
#mutating data
iris_mut <- iris %>%
mutate(petal_perimeter = 2(Petal.Length + Petal.Width))
#mutating data
iris_mut <- iris %>%
mutate(petal_perimeter = 2*Petal.Length + 2*Petal.Width)
View(iris_mut)
#rename columns
iris_rename <- iris_mut %>%
rename(Petal.Perimeter = petal_perimeter)
View(iris_rename)
#selecting columns
iris_slct <- iris_rename %>%
select(Sepal.Length, Sepal.Width, Petal.Perimeter)
View(iris_slct)
#slicing up
iris_slice <- iris_slct %>%
slice(1:15)
View(iris_slice)
x <- c(2, 4, 6)
y <- c(1, 3, 5)
z <- x+y
z
mean(rpois(10000,4) <=2)
# Parameters
lambda <- 21  # Average number of calls per hour
k <- 39       # Number of calls to find probability for
# Calculate P(X = 39)
prob <- dpois(k, lambda)
# Output the result
print(prob)
dpois(39,21)
n <- 1600
k <- 30
p <- 0.02
prob <- dbinom(k, n, p)
print(prob)
add_nums <- function(a, b){
return(a+b)
}
result <- add_nums(5,6)
add_nums(5,6)
add_nums(-4.3, 2.5)
x <- c(2,4,6)
print(x[2])
mean(c(1,NA,3), na.rm = TRUE)
NA == NA
x.dtypes
iris.dtypes
iris_arr.dtypes
cars.dtypes
load(cars)
cars
df <- cars
df.dtypes
df(dtypes)
str(df)
str(iris)
df[df$speed > 10, ]
df[speed>30]
# Load required packages
if (!require("ChainLadder")) {
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages("ChainLadder")
}
library(ChainLadder)
# Example cumulative paid claims triangle for Commercial Auto (in thousands)
claims_triangle <- as.triangle(matrix(
c(120, 200, 250, 270, 280,
150, 220, 260, 275, NA,
180, 240, 270, NA,  NA,
200, 260, NA,  NA,  NA,
220, NA,  NA,  NA,  NA),
nrow = 5, byrow = TRUE,
dimnames = list(AccidentYear = 2016:2020, DevYear = 1:5)
))
# Chain-Ladder Method
cl_result <- MackChainLadder(claims_triangle)
# Bornhuetter-Ferguson Method
# Assume expected loss ratios (ELR) for each accident year
ELR <- c(0.65, 0.65, 0.65, 0.65, 0.65)
# Assume earned premiums for each accident year (in thousands)
earned_premium <- c(400, 420, 440, 460, 480)
# Calculate expected ultimate claims using ELR
expected_ultimates <- ELR * earned_premium
# Calculate percentage paid to date for each accident year
paid_to_date <- apply(claims_triangle, 1, function(x) tail(na.omit(x), 1))
cum_factors <- cl_result$LatestDevFactors
percent_paid <- 1 / cum_factors
# Bornhuetter-Ferguson ultimate = Paid to date + (1 - % paid) * Expected ultimate
bf_ultimate <- paid_to_date + (1 - percent_paid) * expected_ultimates
# Results comparison
results <- data.frame(
AccidentYear = 2016:2020,
ChainLadder_Ultimate = cl_result$FullTriangle[, ncol(cl_result$FullTriangle)],
BF_Ultimate = bf_ultimate
)
print("Comparison of Chain-Ladder and Bornhuetter-Ferguson Ultimate Estimates:")
print(results)
View(cl_result)
cursor
rm(mydata1)
library(arules)
install.packages("arules")
restart
reboot
boot
install.packages("arules")
install.packages("arules",
repos = c("https://mhahsler.r-universe.dev",
"https://cloud.r-project.org/"))
install.packages("arules")
df.head(5)
library(datasets)
df = data("UCBAdmissions")
df.head(5)
library(datasets)
df = data("UCBAdmissions")
df
library(datasets)
df = data("UCBAdmissions")
view(df)
library(datasets)
df = data("UCBAdmissions")
View(df)
library(datasets)
data("UCBAdmissions")
library(datasets)
data("UCBAdmissions")
force(UCBAdmissions)
library(datasets)
data("UCBAdmissions")
str(UCBAdmissions)
library(datasets)
data("UCBAdmissions")
str(UCBAdmissions)
sum(UCBAdmissions)
margin.table(UCBAdmissions, 1)
margin.table(UCBAdmissions, 2:3)
library(datasets)
data("UCBAdmissions")
str(UCBAdmissions)
sum(UCBAdmissions)
margin.table(UCBAdmissions, 1)
margin.table(UCBAdmissions, 2:3)
head(UCBAdmissions)
str(UCBAdmissions)
sum(UCBAdmissions)
margin.table(UCBAdmissions, 1)
margin.table(UCBAdmissions, 2:3)
str(UCBAdmissions)
sum(UCBAdmissions)
margin.table(UCBAdmissions, 1)
margin.table(UCBAdmissions, 2:3)
require(graphics)
## Data aggregated over departments
apply(UCBAdmissions, c(1, 2), sum)
## Data aggregated over departments
apply(UCBAdmissions, c(1, 2), sum)
# 2. Create a contingency table to summarize the number of admissions and rejections by gender and department
#    (marginal table collapsing over Department)
admit_by_gender <- margin.table(UCBAdmissions, c(1, 2))  # 1=Admit, 2=Gender
print(admit_by_gender)
# 2. Create a contingency table to summarize the number of admissions and rejections by gender and department
#    (marginal table collapsing over Department)
admit_by_gender <- margin.table(UCBAdmissions, c(1, 2))  # 1=Admit, 2=Gender
print(admit_by_gender)
# admission rates by gender
prop.table(admit_by_gender, margin = 2)
# 3. Perform Chi-Square Test for Independence
chi_test <- chisq.test(admit_by_gender)
# 3. Perform Chi-Square Test for Independence
chi_test <- chisq.test(admit_by_gender)
print(chi_test)
# 3. Perform Chi-Square Test for Independence
chi_test <- chisq.test(admit_by_gender, correct = FALSE)
print(chi_test)
# 2. Create a contingency table to summarize the number of admissions and rejections by gender and department
#    (marginal table collapsing over Department)
by_gender <- margin.table(UCBAdmissions, c(1, 2))  # 1=Admit, 2=Gender
print(by_gender)
# admission rates by gender
prop.table(by_gender, margin = 2)
# 3. Perform Chi-Square Test for Independence
chi_test <- chisq.test(by_gender, correct = FALSE)
print(chi_test)
by_dept <- apply(UCBAdmissions,MARGIN=c(1,3),FUN=sum)
print(by_dept)
by_dept <- apply(UCBAdmissions,MARGIN=c(1,3),FUN=sum)
print(by_dept)
#convert to table
dept_admit = t(by_dept)
print(dept_admit)
by_dept <- apply(UCBAdmissions,MARGIN=c(1,3),FUN=sum)
print(by_dept)
#convert to contigency table
dept_admit = t(by_dept)
print(dept_admit)
#admission rates by dept
prop.table(DeptAdmit,margin=1)
by_dept <- apply(UCBAdmissions,MARGIN=c(1,3),FUN=sum)
print(by_dept)
#convert to contigency table
dept_admit = t(by_dept)
print(dept_admit)
#admission rates by dept
prop.table(by_dept,margin=1)
by_dept <- apply(UCBAdmissions,MARGIN=c(1,3),FUN=sum)
print(by_dept)
#convert to contigency table
dept_admit = t(by_dept)
print(dept_admit)
#admission rates by dept
prop.table(dept_admit,margin=1)
chisq.test(dept_admit,correct = FALSE)
# 2. Create a contingency table to summarize the number of admissions and rejections by gender and department
gender_admit <- margin.table(UCBAdmissions, c(1, 2))  # 1=Admit, 2=Gender
print(by_gender)
# admission rates by gender
prop.table(by_gender, margin = 2)
# 3. Perform Chi-Square Test for Independence
chi_test <- chisq.test(gender_admit, correct = FALSE)
print(chi_test)
# 3. Perform Chi-Square Test to assess if there is an association between gender and admission status
chi_test <- chisq.test(gender_admit, correct = FALSE)
print(chi_test)
summary(HairEyeColor)
summary(HairEyeColor)
HairEyeColor
eye_freq <- margin.table(HairEyeColor, 3)  # Margin 3 corresponds to Eye color
eye_freq
# Perform Chi-Square Goodness-of-Fit Test
chi_test <- chisq.test(eye_freq)
# Display results
chi_test
# Extract components
chi_test$statistic   # Chi-Square statistic
chi_test$parameter   # Degrees of freedom
chi_test$p.value     # p-value
chi_test$expected    # Expected counts
# Perform Chi-Square Goodness-of-Fit Test
chi_test <- chisq.test(eye_freq)
# Display results
chi_test
# 2. Create a contingency table to summarize the number of admissions and rejections by gender and department
gender_admit <- margin.table(UCBAdmissions, c(1, 2))  # 1=Admit, 2=Gender
# admission rates by gender
prop.table(by_gender, margin = 2)
# 2. Create a contingency table to summarize the number of admissions and rejections by gender and department
gender_admit <- margin.table(UCBAdmissions, c(1, 2))  # 1=Admit, 2=Gender
print(gender_admit)
# admission rates by gender
prop.table(by_gender, margin = 2)
# 2. Create a contingency table to summarize the number of admissions and rejections by gender and department
gender_admit <- margin.table(UCBAdmissions, c(1, 2))  # 1=Admit, 2=Gender
print(gender_admit)
# admission rates by gender
prop.table(gender_admit, margin = 2)
library(forecast)
library(tseries)
# 1. Load the data
# Make sure you've downloaded 'DailyTotalFemaleBirths.csv' from Kaggle and saved in working directory.
data <- read.csv("DailyTotalFemaleBirths.csv")
# Inspect the data structure
str(data)
q()
library(forecast)
library(tseries)
# 1. Load the data
# Make sure you've downloaded 'DailyTotalFemaleBirths.csv' from Kaggle and saved in working directory.
data <- read.csv("DailyTotalFemaleBirths.csv")
setwd("~/documents/github/msc_data-science/dsc_804-advanced_statistics/timeseries")
# 1. Load the data
# Make sure you've downloaded 'DailyTotalFemaleBirths.csv' from Kaggle and saved in working directory.
data <- read.csv("DailyTotalFemaleBirths.csv")
# 1. Load the data
# Make sure you've downloaded 'DailyTotalFemaleBirths.csv' from Kaggle and saved in working directory.
data <- read.csv("DailyTotalFemaleBirth.csv")
# Inspect the data structure
str(data)
head(data)
# The dataset typically has columns: 'Date' and 'Births'.
# Convert the 'Date' column to Date class
data$Date <- as.Date(data$Date, format="%Y-%m-%d")
# Create a time series object
# Since these are daily births for 1959, frequency = 365 is often used for daily data (non-leap year)
births_ts <- ts(data$Births, start=c(1959,1), frequency=365)
# 2. Test for stationarity
# We can use the Augmented Dickey-Fuller (ADF) test:
adf_result <- adf.test(births_ts)
# Load required libraries
library(terra)
# Define the URL for the shapefile (Natural Earth Admin 0 Countries)
url <- "https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip"
output_zip <- "ne_countries.zip"
output_dir <- "natural_earth"
# Download the shapefile
download.file(url, output_zip, mode = "wb")
# Unzip the downloaded file
unzip(output_zip, exdir = output_dir)
# Load the shapefile
shapefile_path <- file.path(output_dir, "ne_110m_admin_0_countries.shp")
countries <- vect(shapefile_path)
# Inspect the data
print(countries)
# Plot the shapefile
plot(countries, col = "lightblue", border = "darkblue", main = "World Map (Natural Earth Data)")
# Compute adjacency matrix (Rook's case)
adj_matrix <- adjacent(countries, "rook", pairs = FALSE)
# View part of the adjacency matrix
print(adj_matrix)
print(adj_matrix[1:6, 1:6])
# Compute centroids of polygons
centroids <- centroids(countries)
# Compute distance-based neighbors (e.g., 100 km and 500 km thresholds)
dist_100 <- nearby(centroids, distance = 100000)  # 100 km
dist_500 <- nearby(centroids, distance = 500000)  # 500 km
# Compute k-nearest neighbors (e.g., 3 and 6 neighbors)
k3_neighbors <- nearby(centroids, k = 3)
k6_neighbors <- nearby(centroids, k = 6)
# Define a plotting function
plot_links <- function(nb, label) {
plot(countries, col = "gray", border = "white")
c1 <- centroids[nb[, 1], ]
c2 <- centroids[nb[, 2], ]
lines(c1, c2, col = "red", lwd = 2)
title(label)
}
# Plot adjacency, distance-based, and nearest neighbor influences
par(mfrow = c(1, 1))
plot_links(nb = adjacent(countries, "rook"), label = "Adjacency (Rook's Case)")
plot_links(nb = dist_500, label = "Distance-based (500 km)")
plot_links(nb = k6_neighbors, label = "6-Nearest Neighbors")
# Load required libraries
library(spatstat)
# Create a non-regular polygonal boundary
coords <- matrix(c(
0, 0,
10, 0,
8, 5,
10, 10,
5, 7,
2, 10,
0, 5,
0, 0
), ncol = 2, byrow = TRUE)
# Define the polygon as an owin object
irregular_area <- owin(poly = list(x = coords[,1], y = coords[,2]))
# Simulate a random point pattern in the irregular area
set.seed(42)
nests <- rpoispp(lambda = 0.5, win = irregular_area)
# Perform density-based analysis (Kernel Density Estimation)
nest_density <- density(nests)
plot(nest_density, main = "Kernel Density of Bird Nests")
# Perform distance-based analysis (K-function)
k_result <- Kest(nests)
plot(k_result, main = "K-function for Bird Nests")
#load package
library(spdep)
# simulate dataset
n <- 10
coords <- cbind(runif(n, 0, 100), runif(n, 0, 100))
y <- sample(1:15, n, replace = TRUE)
# Create a Spatial Weights Matrix Define neighboring relationships using the -nearest neighbors method:
nb &lt;- knn2nb(knearneigh(coords, k = 3))
wm &lt;- nb2mat(nb, style = "W", zero.policy = TRUE)
# Compute Spatially Lagged Values Aggregate the values of neighboring polygons to compute spatially lagged values:
ms &lt;- cbind(id = rep(1:n, each = n), y = rep(y, each = n), value = as.vector(wm * y))
ms &lt;- ms[ms[, 3] &gt; 0, ]
ams &lt;- aggregate(ms[, 2:3], list(ms[, 1]), FUN = mean)
ams &lt;- ams[, -1]
colnames(ams) &lt;- c("y", "spatially lagged y")
# Visualize with Moran Scatter Plot The Moran scatter plot illustrates the relationship between observed values and spatially lagged values
plot(ams, pch = 20, col = "red", cex = 2,
xlab = "Observed y",
ylab = "Spatially lagged y",
main = "Moran Scatter Plot")
reg &lt;- lm(ams[, 2] ~ ams[, 1])
#load package
library(spdep)
install.packages("spdep")
